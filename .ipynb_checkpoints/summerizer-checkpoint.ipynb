{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d37ff4a",
   "metadata": {},
   "source": [
    "# A python script to summerize blog post using python and Open AI's GPT-3 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42aea8f2",
   "metadata": {},
   "source": [
    "Imports the necessary modules openai and requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b5ac1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1bfce4",
   "metadata": {},
   "source": [
    "The code below sets the OpenAI API key using the api_key property of the openai module. This is a private key that allows you to access the OpenAI API and use their language models. You'll need to obtain your own API key in order to use this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50dec4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the OpenAI API client read from a file in the root folder.\n",
    "openai.api_key = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a3b272",
   "metadata": {},
   "source": [
    "The **url** variable which holds the URL of the article that we want to summarize. In this case, it's set to \"https://lawandspace.com/negotiating-their-future-a-marshallese-geography-of-us-policy/\", which is a placeholder URL for demonstration purposes. **You would need to replace this with the URL of the actual article that you want to summarize.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2982b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URL of the article to summarize\n",
    "url = \"https://lawandspace.com/negotiating-their-future-a-marshallese-geography-of-us-policy/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61cfb1a",
   "metadata": {},
   "source": [
    "## For Single Url\n",
    "Code to Generate summery single url by taking all the paragraphs and heading within the blog/article into consideration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7964b43",
   "metadata": {},
   "outputs": [
    {
     "ename": "APIError",
     "evalue": "Internal server error {\n    \"error\": {\n        \"message\": \"Internal server error\",\n        \"type\": \"auth_subrequest_error\",\n        \"param\": null,\n        \"code\": \"internal_error\"\n    }\n}\n 500 {'error': {'message': 'Internal server error', 'type': 'auth_subrequest_error', 'param': None, 'code': 'internal_error'}} {'Date': 'Tue, 21 Feb 2023 09:53:14 GMT', 'Content-Type': 'application/json; charset=utf-8', 'Content-Length': '166', 'Connection': 'keep-alive', 'Vary': 'Origin', 'X-Request-Id': 'f22739e19cdb464f262d463d7cb62091', 'Strict-Transport-Security': 'max-age=15724800; includeSubDomains'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAPIError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/r0/2m99vr9541v8dlydfddwbqkr0000gn/T/ipykernel_972/1831668211.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Generate the summary for the selected paragraphs and headers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mprompt_with_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcontent_to_summarize\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\\nSummary:\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m response = openai.Completion.create(\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_engine\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt_with_content\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/openai/api_resources/completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    151\u001b[0m         )\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         )\n\u001b[0;32m--> 226\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    617\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m             return (\n\u001b[0;32m--> 619\u001b[0;31m                 self._interpret_response_line(\n\u001b[0m\u001b[1;32m    620\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 679\u001b[0;31m             raise self.handle_error_response(\n\u001b[0m\u001b[1;32m    680\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m             )\n",
      "\u001b[0;31mAPIError\u001b[0m: Internal server error {\n    \"error\": {\n        \"message\": \"Internal server error\",\n        \"type\": \"auth_subrequest_error\",\n        \"param\": null,\n        \"code\": \"internal_error\"\n    }\n}\n 500 {'error': {'message': 'Internal server error', 'type': 'auth_subrequest_error', 'param': None, 'code': 'internal_error'}} {'Date': 'Tue, 21 Feb 2023 09:53:14 GMT', 'Content-Type': 'application/json; charset=utf-8', 'Content-Length': '166', 'Connection': 'keep-alive', 'Vary': 'Origin', 'X-Request-Id': 'f22739e19cdb464f262d463d7cb62091', 'Strict-Transport-Security': 'max-age=15724800; includeSubDomains'}"
     ]
    }
   ],
   "source": [
    "# Get the text content of the article\n",
    "response = requests.get(url)\n",
    "content = response.text\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "# Extract the first five paragraphs and first two headers\n",
    "paragraphs = soup.find_all('p')[:5]\n",
    "headers = soup.find_all(['h1', 'h2'])[:2]\n",
    "\n",
    "# Combine the paragraphs and headers into a single string\n",
    "content_to_summarize = ''\n",
    "for element in headers + paragraphs:\n",
    "    content_to_summarize += element.text + '\\n\\n'\n",
    "\n",
    "# Set up the GPT-3 API parameters\n",
    "model_engine = \"text-davinci-002\"\n",
    "max_tokens = 256\n",
    "temperature = 0.7\n",
    "n = 1\n",
    "stop = None\n",
    "prompt = \"Please summarize the following article:\\n\\n\"\n",
    "\n",
    "# Generate the summary for the selected paragraphs and headers\n",
    "prompt_with_content = prompt + content_to_summarize + \"\\n\\nSummary:\"\n",
    "response = openai.Completion.create(\n",
    "    engine=model_engine,\n",
    "    prompt=prompt_with_content,\n",
    "    max_tokens=max_tokens,\n",
    "    temperature=temperature,\n",
    "    n=n,\n",
    "    stop=stop\n",
    ")\n",
    "summary = response.choices[0].text.strip()\n",
    "\n",
    "# Print the summary\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40282c47",
   "metadata": {},
   "source": [
    "## For multiple Urls\n",
    "Code to Generate summery of multiple urls by taking all the paragraphs and heading within the blog/article into consideration. The urls need to be passed as an array in **urls** variable like below. **You would need to replace this with the URL of the actual article that you want to summarize.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea87cd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\"https://lawandspace.com/negotiating-their-future-a-marshallese-geography-of-us-policy/\", \n",
    "        \"https://immigrationimpact.com/2023/02/08/awaiting-court-dates-notice-to-appear/\", \n",
    "        \"https://www.theguardian.com/uk-news/2023/feb/04/rishi-sunak-stop-deportation-appeals-people-uk-small-boats\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69cef71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the GPT-3 API parameters\n",
    "model_engine = \"text-davinci-002\"\n",
    "max_tokens = 256\n",
    "temperature = 0.7\n",
    "n = 1\n",
    "stop = None\n",
    "prompt = \"Please summarize the following article:\\n\\n\"\n",
    "\n",
    "# Loop over each URL and generate a summary\n",
    "for url in urls:\n",
    "    # Get the text content of the article\n",
    "    response = requests.get(url)\n",
    "    content = response.text\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "    # Extract the first five paragraphs and first two headers\n",
    "    paragraphs = soup.find_all('p')[:5]\n",
    "    headers = soup.find_all(['h1', 'h2'])[:2]\n",
    "\n",
    "    # Combine the paragraphs and headers into a single string\n",
    "    content_to_summarize = ''\n",
    "    for element in headers + paragraphs:\n",
    "        content_to_summarize += element.text + '\\n\\n'\n",
    "\n",
    "    # Generate the summary for the selected paragraphs and headers\n",
    "    prompt_with_content = prompt + content_to_summarize + \"\\n\\nSummary:\"\n",
    "    response = openai.Completion.create(\n",
    "        engine=model_engine,\n",
    "        prompt=prompt_with_content,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        n=n,\n",
    "        stop=stop\n",
    "    )\n",
    "    summary = response.choices[0].text.strip()\n",
    "\n",
    "    # Print the summary for the current URL\n",
    "    print(f\"Summary for {url}: {summary}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3b319b",
   "metadata": {},
   "source": [
    "## Short discription of api parameters used\n",
    "\n",
    "- **engine**: This specifies the GPT-3 engine to use for generating the summary. The text-davinci-002 engine is the most capable and is often used for natural language generation tasks.\n",
    "\n",
    "- **prompt**: This is the prompt that is provided to the GPT-3 model. In this case, we are providing a prompt that asks the model to summarize the given article.\n",
    "\n",
    "- **max_tokens**: This parameter sets the maximum number of tokens (words or sub-words) to be generated by the model. In this case, we are limiting the summary to 256 tokens.\n",
    "\n",
    "- **temperature**: This parameter controls the \"creativity\" of the model. A lower temperature value will generate more conservative and predictable responses, while a higher temperature value will generate more creative and unpredictable responses. In this case, we are using a temperature of 0.7, which is a good balance between predictability and creativity.\n",
    "\n",
    "- **n**: This parameter specifies the number of different completions to generate for the given prompt. In this case, we are generating only one completion.\n",
    "\n",
    "- **stop**: This parameter specifies a string that, if generated by the model, will indicate that the summary is complete. In this case, we are not using a stop sequence, so the model will generate a summary of the maximum length specified by max_tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ab569a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
